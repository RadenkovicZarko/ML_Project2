{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "vhWzYaata8pO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing data"
      ],
      "metadata": {
        "id": "DNLbQMoQbJOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "train_data = train_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "test_data = test_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "\n",
        "train_data = train_data.dropna()\n",
        "test_data = test_data.dropna()"
      ],
      "metadata": {
        "id": "FHp9DnaRbFz7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing "
      ],
      "metadata": {
        "id": "lYVuNvVlbOrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# transforming categorical data\n",
        "\n",
        "# Sex\n",
        "encoder.fit(train_data['Sex'])\n",
        "train_data['Sex'] = encoder.transform(train_data['Sex'])\n",
        "test_data['Sex'] = encoder.transform(test_data['Sex'])\n",
        "\n",
        "#Embarked\n",
        "encoder.fit(train_data['Embarked'])\n",
        "train_data['Embarked'] = encoder.transform(train_data['Embarked'])\n",
        "test_data['Embarked'] = encoder.transform(test_data['Embarked'])"
      ],
      "metadata": {
        "id": "r2r2yBCsbTN5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# standardize data if needed (not needed for trees, but can be useful for neural networks)\n",
        "def standardize():\n",
        "  for column_name, _ in train_data.iteritems():\n",
        "      if column_name != 'Survived':\n",
        "        column_train = train_data[column_name].to_numpy().reshape(-1,1)\n",
        "        scaler.fit(column_train)\n",
        "        # train data\n",
        "        scaled_train = scaler.transform(column_train)\n",
        "        train_data[column_name] = scaled_train\n",
        "        # test data\n",
        "        column_test = test_data[column_name].to_numpy().reshape(-1,1)\n",
        "        scaled_test = scaler.transform(column_test)\n",
        "        test_data[column_name] = scaled_test\n",
        "\n",
        "standardize()\n",
        "x_train = train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y_train = train_data['Survived']\n",
        "x_test = test_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y_test = test_data['Survived']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D9agAlzbWR6",
        "outputId": "842a32ef-7727-4f44-882f-5950dd2e8300"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cf87f16166b6>:7: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column_name, _ in train_data.iteritems():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network"
      ],
      "metadata": {
        "id": "wV2lqSZ4gh1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SqqDXZCKatnH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, input_size, output_size, learning_rate=0.001, nb_epochs=16, batch_size=128):  \n",
        "    # Parametri mreze\n",
        "    self.learning_rate = learning_rate\n",
        "    self.nb_epochs = nb_epochs\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    # Parametri arhitekture\n",
        "    self.nb_input = input_size\n",
        "    self.nb_hidden1 = 256  # 1st layer number of neurons\n",
        "    self.nb_hidden2 = 256  # 2nd layer number of neurons\n",
        "    self.nb_classes = output_size   # dead/alive\n",
        "    # Sama mreza\n",
        "    self.w = {\n",
        "        '1': tf.Variable(tf.random.normal([self.nb_input, self.nb_hidden1], dtype=tf.float64)),\n",
        "        '2': tf.Variable(tf.random.normal([self.nb_hidden1, self.nb_hidden2], dtype=tf.float64)),\n",
        "        'out': tf.Variable(tf.random.normal([self.nb_hidden2, self.nb_classes], dtype=tf.float64))\n",
        "    }\n",
        "\n",
        "    self.b = {\n",
        "        '1': tf.Variable(tf.random.normal([self.nb_hidden1], dtype=tf.float64)),\n",
        "        '2': tf.Variable(tf.random.normal([self.nb_hidden2], dtype=tf.float64)),\n",
        "        'out': tf.Variable(tf.random.normal([self.nb_classes], dtype=tf.float64))\n",
        "    }\n",
        "    # Aktivacione funkcije\n",
        "    self.activation = {\n",
        "        '1': tf.nn.relu,\n",
        "        '2': tf.nn.relu,\n",
        "        'out': tf.nn.softmax\n",
        "    }\n",
        "\n",
        "    self.opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  def fit(self, x_train, y_train):\n",
        "    nb_train = len(y_train)\n",
        "    for epoch in range(self.nb_epochs):\n",
        "        epoch_loss = 0\n",
        "        nb_batches = int(nb_train / self.batch_size)\n",
        "        for i in range(nb_batches):\n",
        "            x = x_train[i*self.batch_size : (i+1)*self.batch_size]\n",
        "            y = y_train[i*self.batch_size : (i+1)*self.batch_size]\n",
        "            y_onehot = tf.one_hot(y, self.nb_classes)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                _, z_out = self.predict(x)\n",
        "\n",
        "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_out, labels=y_onehot))\n",
        "\n",
        "            w1_g, w2_g, wout_g, b1_g, b2_g, bout_g = tape.gradient(loss, [self.w['1'], self.w['2'], self.w['out'], self.b['1'], self.b['2'], self.b['out']])\n",
        "\n",
        "            self.opt.apply_gradients(zip([w1_g, w2_g, wout_g, b1_g, b2_g, bout_g], [self.w['1'], self.w['2'], self.w['out'], self.b['1'], self.b['2'], self.b['out']]))\n",
        "\n",
        "            epoch_loss += loss\n",
        "\n",
        "        # U svakoj epohi ispisujemo proseƒçan loss.\n",
        "        epoch_loss /= nb_train\n",
        "        if(epoch+1)%64 == 0:\n",
        "          print(f'Epoch: {epoch+1}/{self.nb_epochs}| Avg loss: {epoch_loss:.5f}')\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "      l1 = tf.add(tf.matmul(x, self.w['1']), self.b['1'])\n",
        "      a1 = self.activation['1'](l1)\n",
        "      l2 = tf.add(tf.matmul(a1, self.w['2']), self.b['2'])\n",
        "      a2 = self.activation['2'](l2)\n",
        "      l_out = tf.add(tf.matmul(a2, self.w['out']), self.b['out'])\n",
        "      out = self.activation['out'](l_out)\n",
        "\n",
        "      pred = tf.argmax(out, 1)\n",
        "\n",
        "      return pred, l_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "n7_LTn19glYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(len(x_train.columns), y_train.nunique(), nb_epochs=512)\n",
        "nn.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BIT4bT2hGOO",
        "outputId": "17975ba2-1cf1-47cf-b04e-6e644fbcdfe6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 64/512| Avg loss: 0.00649\n",
            "Epoch: 128/512| Avg loss: 0.00406\n",
            "Epoch: 192/512| Avg loss: 0.00303\n",
            "Epoch: 256/512| Avg loss: 0.00319\n",
            "Epoch: 320/512| Avg loss: 0.00263\n",
            "Epoch: 384/512| Avg loss: 0.00238\n",
            "Epoch: 448/512| Avg loss: 0.00234\n",
            "Epoch: 512/512| Avg loss: 0.00268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "wNUG5lcjiXDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred, _ = nn.predict(x_test)\n",
        "\n",
        "pred_correct = tf.equal(pred, y_test)\n",
        "accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n",
        "\n",
        "print(f'Test acc: {accuracy:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYzCtxsxg4tR",
        "outputId": "681c765b-598d-410d-b4f1-197c628dafb0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.734\n"
          ]
        }
      ]
    }
  ]
}